# import app.pages.quantum_brain_ollama.llm as llm
import os
import streamlit as st

# import app.state_manager as state_manager
import logging
import time
from pathlib import Path  # Import Path from pathlib for file path handling
import re
import json
import tiktoken
from app.history import ChatHistory

from st_copy_to_clipboard import st_copy_to_clipboard
from logging.handlers import (
    RotatingFileHandler,
)

# Define log file and handler
log_file = "app.log"
handler = RotatingFileHandler(log_file, maxBytes=102400, backupCount=0)

# Configure logger
logger = logging.getLogger()

# Remove existing handlers to avoid duplicates
for h in logger.handlers[:]:
    logger.removeHandler(h)

# Add new handler only once
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger.setLevel(logging.INFO)
logger.addHandler(handler)





@st.dialog("ResearchFlow üîç Setup Error üö®")
def setup_error_dialog(info: str):
    """
    Displays an error message and stops the execution of the Streamlit app.

    Parameters:
    info (str): The error message to be displayed.
    """
    st.write(info)
    st.stop()


# Function to simulate streaming data
# Function to stream non-table content and display the table at once
def stream_ai_response(content):
    """
    Simulate streaming of AI response by displaying content word by word with a delay.

    Args:
        content (str): The content to be streamed.

    Returns:
        str: The fully streamed content.
    """
    """Simulate streaming of AI response."""
    message_container = st.empty()  # Reserve space for the AI message
    streamed_content = ""
    for word in content.split(" "):  # Stream content word by word
        streamed_content += word + " "
        # Update the container with streamed content
        message_container.markdown(
            f'<div class="custom-ai-content">{streamed_content}</div>',
            unsafe_allow_html=True,
        )
        time.sleep(0.02)  # Simulate a delay for each word
    return streamed_content


# Print the response of the user
def print_ai_response(ai_response, selected_chat, history, user_input_id):
    """
    Prints the AI's response in a chat message with an avatar and updates the session state.

    Args:
        ai_response (str): The response generated by the AI.

    Returns:
        None
    """

    # Placeholder for AI's streamed response
    with st.chat_message("ai", avatar="üîç"):
        ai_streamed_response = stream_ai_response(f"{ai_response}")
        # Add the complete streamed response to session state
        history.add_ai_response(selected_chat, user_input_id, ai_streamed_response)
        # Update the conversation history


def render_latex(content: str):
    """
    Pre-process AI response to properly render LaTeX blocks and inline math.
    """
    # Replace LaTeX block delimiters: \[...\] -> $$...$$
    content = re.sub(r"\\\[(.*?)\\\]", r"$$\1$$", content, flags=re.DOTALL)

    # Replace LaTeX inline delimiters: \( ... \) -> $...$
    content = re.sub(r"\\\((.*?)\\\)", r"$\1$", content)

    return content



def on_regenerate_response(user_input: str, 
                           message_idx: int, 
                        #    main_container, 
                           regenerate_response, 
                           history, 
                           selected_chat, 
                           user_input_id, 
                           max_version):
    """
    Regenerate the response based on the user input.

    Args:
        user_input (str): The user input.

    Returns:
        str: The regenerated response.
    """
    # with main_container:
    with st.spinner("Regenerating...", show_time=True):
        old_response = st.session_state["messages"].pop(message_idx)["content"]
        logging.info(f"Old response: {old_response}")
        # Regenerate response
        new_ai_response = regenerate_response(json.loads(user_input))
        

        with st.chat_message("ai", avatar="üîç"):
            ai_streamed_response = stream_ai_response(
                f"{new_ai_response}"
            )

        st.session_state["messages"] = [
            {
                "role": "ai",
                "content": "Hello! I am here to help you with your tasks. How can I help you today?",
                "version": 0,
                "user_input_id": "",
            }
        ]

        # Update the conversation history
        
        history.add_ai_response(
            selected_chat,
            user_input_id,
            ai_streamed_response,
            version=max_version + 1,
        )
        history.load_chat_into_session_state(selected_chat)
        st.rerun()


    

def display_messages(
    selected_chat: str,
    regenerate_response: callable,
    history: ChatHistory,
    # main_container,
):
    """Display messages in the chat container."""
    if st.session_state["new_chat"]:
            
            st.session_state["messages"] = [
                {
                    "role": "ai",
                    "content": "Hello! I am here to help you with your Research. How can I help you today?",
                    "version": 0,
                    "user_input_id": "",
                }
            ]
            default_message = (
                f"Hello! I am here to help you with your Research. How can I help you today? "
            )
            
            
            st.chat_message("ai", avatar="üîç").markdown(
                default_message, unsafe_allow_html=False
            )
            
            
            return
            
    messages = st.session_state["messages"]

    for i, message in enumerate(messages):
        default_message = (
            "Hello! I am here to help you with your tasks. How can I help you today?"
        )
        

        # if (
        #     message["role"] == "ai" and message["version"] == 0
        # ) or selected_chat == "new_chat":
        #     st.chat_message("ai", avatar="üîç").markdown(
        #         default_message, unsafe_allow_html=False
        #     )
        #     continue
        

        if message["role"] == "ai":
            content = render_latex(message["content"])
            st.chat_message("ai", avatar="üîç").write(content, unsafe_allow_html=True)
            

            user_input = message["user_input"]
            user_input_id = history.get_user_input_id(selected_chat, user_input)
            ai_responses = history.get_ai_responses(user_input_id)
            max_version = len(ai_responses)
            col1, _ = st.columns([5, 4])
            current_version = message["version"]

            with col1:
                if max_version > 1:
                    (
                        regenerator,
                        prev_btn,
                        label,
                        next_btn,
                        copy,
                        
                    ) = st.columns([1, 1, 2, 1, 1.5, ], gap="medium")
                else:
                    
                    regenerator, copy,  = st.columns([ 1, 2])

                

                

                # Regenerate button
                if message["content"] != default_message:
                    if regenerator.button(
                        "üîÑ",
                        key=f"regenerate_{i}_chat_{selected_chat}",
                        use_container_width=True,
                        help="Regenerate",
                        
                    ):
                        on_regenerate_response(user_input, 
                                               i, 
                                               #main_container, 
                                               regenerate_response, 
                                               history, 
                                               selected_chat, 
                                               user_input_id, 
                                               max_version)
                
                
                with copy :
                    st_copy_to_clipboard(message["content"],
                                         before_copy_label = "üìã",
                                        after_copy_label = "‚úÖ",
                                        show_text = False, key=f"copy_{i}_chat_{selected_chat}")

                # Navigation buttons for previous and next versions
                if max_version > 1:
                    if prev_btn.button(
                        "¬´",
                        key=f"prev_{current_version}_{message['version']}_{user_input_id}",
                        use_container_width=True,
                    ):
                        if current_version > 1:
                            # Load the previous version
                            idx = current_version - 1
                            idx = (idx - 1) % max_version
                            messages[i]["content"] = ai_responses[idx][0]
                            messages[i]["version"] = ai_responses[idx][1]
                            st.session_state["messages"] = messages

                            st.rerun()

                    # Current version indicator
                    label.markdown(
                        f"<div style='text-align: center; padding-top: 6px; font-weight: bold;'>{current_version} / {max_version}</div>",
                        unsafe_allow_html=True,
                    )

                    if next_btn.button(
                        "¬ª",
                        key=f"next_{current_version}_{message['version']}_{user_input_id}",
                        use_container_width=True,
                    ):
                        # Load the next version (if available)
                        if current_version < max_version:
                            idx = current_version - 1
                            idx = (idx + 1) % max_version
                            logging.info(f"Next version index: {idx}")

                            messages[i]["content"] = ai_responses[idx][0]
                            messages[i]["version"] = ai_responses[idx][1]
                            st.session_state["messages"] = messages

                            st.rerun()


                
                        
                            
                    
        elif message["role"] == "user":
            # A unique key for the edit state of the message
            edit_key = f"editing_{i}"
            message["idx"] = i
            logging.info(f"Message: {message}")
            logging.info(f"message_idx: {i}")

            # Check if the edit button was clicked or if the edit state is active
            if st.session_state.get(edit_key, False):
                # Show editable text area and buttons
                new_message = st.text_area(
                    "Edit your message",
                    message["content"],
                    max_chars=2000,
                    key=f"edit_message_{i}",
                )

                def on_edit_submit(ola_message = message):
                    user_input_idx = ola_message["idx"]
                    ai_idx = user_input_idx + 1

                    logging.info(f"Editing message at index: {user_input_idx}")
                    logging.info(f"Old message: {ola_message}")

                    # Update the edited user input
                    st.session_state["messages"][user_input_idx]["content"] = new_message

                    # Prepare the updated input for saving and regeneration
                    temp_input = {
                        "Query": new_message,
                        "new_chat": "False",
                        "WebSearch": message.get("web_search", ""),
                        "AdvanceSearch": message.get("advanced_search", ""),
                    }

                    user_input_id = ola_message["user_input_id"]
                    history.update_user_input(user_input_id, json.dumps(temp_input))

                    max_version = len(history.get_ai_responses(user_input_id))

                    # Regenerate AI response
                    #with main_container:
                    with st.spinner("Regenerating...", show_time=True):
                        old_response = st.session_state["messages"].pop(ai_idx)["content"]
                        logging.info(f"Old response: {old_response}")

                        # Regenerate response
                        new_ai_response = regenerate_response(temp_input)

                        # Stream the new AI response
                        with st.chat_message("ai", avatar="üîç"):
                            ai_streamed_response = stream_ai_response(f"{new_ai_response}")

                        # Update the conversation history
                        history.add_ai_response(
                            selected_chat,
                            user_input_id,
                            ai_streamed_response,
                            version=max_version + 1,
                        )

                        history.load_chat_into_session_state(selected_chat)

                    # Reset the editing state and rerun
                    st.session_state[edit_key] = False
                    st.rerun()

                def on_edit_cancel():
                    st.session_state[edit_key] = False
                    # st.rerun()

                # Show buttons for submitting or canceling edits
                col1, col2, _ = st.columns([1, 1, 6])
                with col1:
                    send = st.button("Send", key=f"send_{i}")
                with col2:
                    st.button("Cancel", key=f"cancel_{i}", on_click=on_edit_cancel)
                if send:
                    on_edit_submit()

            else:
                # Display the edit button
                edit_msg, user = st.columns([0.3, 6])
                with edit_msg:
                    if st.button("üìù", key=f"edit_msg_btn_{i}_{selected_chat}"):
                        st.session_state[edit_key] = True
                        st.session_state["ai_response_idx"] = i + 1
                        st.rerun()
                user.chat_message("user", avatar="images/user.png").write(message["content"])


            

            
               
                    


def count_tokens(text, model="gpt-4"):
    """Count tokens in a string based on model's tokenizer."""
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

def trim_chat_history(
    user_input: str,
    system_prompt: str,
    chatHistory: list,
    max_tokens: int = 2000,
    Dify_mode: bool = False,
):
    """
    Trims the chat history to a maximum length of token by removing the oldest messages.

    Args:
        user_input (str): The user input.
        system_prompt (str): The system prompt.
        chatHistory (list): The list to store the conversation history.

    Returns:
        list: The trimed conversation history.
    """
    logging.info("==============App Utils: Trim Chat History ============")
    # logging.info(f"Chat History: {json.dumps(chatHistory, indent=4)}")

    msg = []
    user_token = count_tokens(user_input)
    system_token = count_tokens(system_prompt)
    total_token = user_token + system_token
    trimmed_history = []
    for message in reversed(chatHistory):
        msg = {}
        if message["role"] == "user":
            msg = {"role": "user", "content": message["content"]}
        else:
            msg = {"role": "assistant", "content": message["content"]}
        msg_token = count_tokens(json.dumps(msg, indent=4), "gpt-4")

        if total_token + msg_token > max_tokens:
            break
        else:

            total_token += msg_token
            trimmed_history.insert(0, msg)
    if not Dify_mode:
        trimmed_history.insert(0, {"role": "system", "content": system_prompt})
        
    
    logging.info(f" Max Request Tokens: {total_token}")
    # logging.info(f"Trimmed history: {json.dumps(trimmed_history, indent=4)}")
    
    
    return json.dumps(trimmed_history, indent=4)



def gen_user_input(selected_chat:str, user_input: str, json_output: bool = False):
    """
    Generate a unique ID for the user input along with the type of query.

    Args:
        user_input (str): The user input.
        json_output (bool): Whether to return the result as a JSON string. Defaults to False.

    Returns:
        dict or str: A dictionary containing the user input and query types, or a JSON string if json_output is True.

    """
    hs = ChatHistory()
    file_info = hs.load_file_info(selected_chat)
    file_name = file_info.get("file_name", "")
    # file_id = file_info.get("file_id", "")
    # f_name = st.session_state["file_info"].get("file_name", "")
    # f_id = st.session_state["file_info"].get("file_id", "")
    result = {}
    

    # if file_name != "" and file_id != "":
    #     result = {
    #         "Query": user_input,
    #         "ResearchPaper": {
    #         "transfer_method": "local_file",
    #         "upload_file_id": file_id,
    #         "type": "document"
    #         },
    #         "Knownledge_Base_Name": file_name,
    #         "new_chat": f"{st.session_state['new_chat']}",
    #     }
    # elif f_name != "" and f_id != "":
    #     result = {
    #         "Query": user_input,
    #         "Paper": {
    #         "transfer_method": "local_file",
    #         "upload_file_id": f_id,
    #         "type": "document"
    #         },
    #         "Knownledge_Base_Name": f_name,
    #         "new_chat": f"{st.session_state['new_chat']}",
    #     }
    # else:
    result = {
        "Query": user_input,
        "new_chat": f"{st.session_state['new_chat']}",
        "Knownledge_Base_Name": file_name,
        
    }
    

    if json_output:
        return json.dumps(result, indent=4)

    return result

def throw_error(error_message: str):
    """
    Display an error message and stop the Streamlit app.

    Args:
        error_message (str): The error message to be displayed.
    """
    st.error(error_message)
    st.stop()

def save_file_mappings(file_mappings):
    """Save the mapping of files to their Dify IDs."""
    with open("app/uploads/file_mappings.json", "w") as f:
        json.dump(file_mappings, f, indent=4)

def delete_all_files(directory: str):
    """
    Delete all files in a directory.

    Args:
        directory (str): The directory containing the files to be deleted.
    """
    for file in os.listdir(directory):
        file_path = os.path.join(directory, file)
        if os.path.isfile(file_path):
            os.remove(file_path)
    file_map = {}
    save_file_mappings(file_map)	
